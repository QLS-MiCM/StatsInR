---
title: "04 - Logistic Regression"
author: 
- name: "Adrien Osakwe"
  affiliation: Workshop Lead

date:  "`r format(Sys.Date(), '%B %d, %Y')`"
output:
   rmdformats::html_clean:
    toc: false
    thumbnails: false
    highlight: kate
    use_bookdown: true
    code_folding: hide
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,eval=TRUE)
```

# Logistic Regression

In this section we will look at logistic regression and see how it helps us extend the linear model to classification tasks.

From a probabilistic perspective, the logistic regression follows a Bernoulli distribution

$$
y_i \sim Bernouilli(\pi_i)
$$

Where $y_i$ is the classification for observation $i$. $\pi_i$ represents the Bernoulli parameter for observation $i$ and is calculated as

$$
\pi_i = \sigma(x\beta) = P(y_i = 1| x)=  \frac{1}{1 + \exp(-x\beta)}
$$

Logistic regression extends the linear model by using the sigmoid/logistic function $\sigma(x\beta)$ which allows it to bound its outputs between 0 and 1.

## Sigmoid Function

```{r}
library(tidyverse)
z <- seq(-5,5,0.1)
sig <- 1/(1+exp(-z))

plot(z,sig,
     xlab = 'XB',
     ylab = 'Sigmoid',
     main = 'Sigmoid Function')
abline(h = 0.5)
abline(v = 0)
```

## Example

We can run an example logistic regression model with the iris dataset

```{r}
#Filtering out versicolor to run a binary example
mini_iris <- iris %>%
  filter(Species != 'versicolor')

ggplot(mini_iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point(size = 3, alpha = 0.7) +  # Add points with some transparency
  theme_minimal() +  # Use a clean theme
  labs(
    title = "Iris Dataset: Sepal Dimensions by Species",
    x = "Sepal Length",
    y = "Sepal Width"
  )
```

Now we can run logistic regression model we do this with the glm function

```{r}
iris.fit <- glm(Species ~ Sepal.Length +  Sepal.Width,
                  family = 'binomial',
                  data = mini_iris)

mini_iris$Prob <- predict.glm(iris.fit,
            mini_iris[,c('Sepal.Length','Sepal.Width')],
            type = 'response')

mini_iris$Pred <- ifelse(mini_iris$Prob >= 0.5,"virginica", "setosa")

mini_iris$Result <- mini_iris$Species == mini_iris$Pred


ggplot(mini_iris, aes(x = Sepal.Length, y = Sepal.Width, color = Result)) +
  geom_point(size = 3, alpha = 0.7) +  # Add points with some transparency
  theme_minimal() +  # Use a clean theme
  labs(
    title = "Iris Dataset: Training Classification Performance",
    x = "Sepal Length",
    y = "Sepal Width"
  )
```

We can compare this to linear regression to see how beneficial the logistic formulation is

```{r}
mini_iris$Num_Species <- ifelse(mini_iris$Species == 'setosa', 0,1)
iris.lm <- lm(Num_Species ~ Sepal.Length +  Sepal.Width,
                  data = mini_iris)

mini_iris$Num_pred <- predict.lm(iris.lm,mini_iris)

summary(mini_iris$Num_pred)

summary(mini_iris$Prob)
```

## Exercise

Here we only made use of sepal length and width to distinguish setosa and virginica samples. However, virginica and versicolor cannot be distinguished by these features. Try using a combination of the features in the iris dataset to train a model that can distinguish virginica from versicolor. Remove setosa samples from the dataset first!

```{r}

```
